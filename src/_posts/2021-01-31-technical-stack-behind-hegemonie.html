---
Title: The Technical Stack
Author: jfsmig
Date: 2021-01-31
Keywords: golang, micro services, observability, opentelemetry, prometheus
Description: The game engine will follow the architectural best practices since day 0.
---
<%inherit file="blog"/>

<h2>API First!</h2>
<p>
  Tens of years of evolution in IT teach us that the boundaries and the
  interactions between the parts of a system are a key part of the
  design of a program. Not only is the efficiency in question but more
  important is the ability to maintain the program. Each part of the system
  is exposed to the others via its API.
</p>
<p>
  Let's be honest: designing a good user interface requires different skills
  than designing a good backend. I have no recent UI skill, and designing a UI
  would end in server-side generated HTML pages. Instead, I dedicate my time to
  the backend: a well-design set of interconnected services, each exposing a
  clean API with the proper instrumentation to secure and observe it.
</p>

<h2>Micro-services architecture</h2>
<p>
  My personal conviction leads me to split the system into isolated services (where
  a "service" stands for a set of network processes serving its API). Despite the
  burden of maintaining connections between the services, the latency introduced
  in the communications, it opens the door to a per-service scalability and
  instrumentation.
</p>
<p>
  Standards appeared, with different trade offs. I chose mine: all the backend code
  is written in ${link("https://golang.org","Golang")}. ${link("https://grpc.io","gRPC")}
  is the middleware shared by all our microservices. The persistence of the data for
  each service currently depend on the type of service, but the trend is to stick on
  either object storage or key/value storage:
</p>
<ul>
  <li>
    maps: the local filesystem is currently used, with one file per map.
    There are plans to locate the maps on an Object Storage platform.
  </li>
  <li>
    events: a local ${link("https://github.org/jfsmig/rocksdb","RocksDB")} database is currently used.
    There are plans for a ${link("https://tikv.org/","TiKV")} storage.
  </li>
  <li>
    region: a local file system is currently used.
    There are plans for a ${link("https://tikv.org/","TiKV")} storage.
  </li>
</ul>

<h2>Local Logging</h2>
<p>
  Should we centralize the logs? No.
</p>
<p>
  It sounds weird, isn't it? The purpose of such a centralization is following the
  journey of a request in the whole distributed system. There are others ways to
  achieve this. I want to place a bet on ${anchor("#observability","observability")}
  instead.
</p>
<p>
  Log traces remain very interesting, I'll never say the opposite. In debug mode
  particularly. Much less in production mode, excepted for abnormal situations
  (slow requests, errors). One trace in the access log per request seems acceptable.
  But no centralization. Just a local dump, 2 or 3 days retention, and log rotation.
</p>

<h2 id="observability">Observability at the core</h2>
<p>
  The fine-grained observability of the execution of the RPC calls is allowed by
  ${link("https://opentelemetry.io","OpenTelemetry")}. Traces are generated in a
  side-car agent. By default, the sandbox installation proposes an all-in-one
  installation with its own Prometheus storage and a dashboard.
</p>

<h2>Best-in-class Authorization &amp; Authentication</h2>
<p>
  The ${link("https://www.ory.sh","ORY")} suite is used for the AAA purpose.
  At the gate, a ${link("https://haproxy.org","HAProxy")} enforces the security policy
  with the help of a ${link("","ORY OathKeeper")}.
  OpenID Connect is used for the Authentication via ${link("https://www.ory.sh/hydra/","ORY Hydra")}.
  A local user registry is available thanks to ${link("https://www.ory.sh/kratos/","ORY Kratos")}, as a source for ORY Hydra.
  The authorization of the users on their cities is achieved with ${link("https://www.ory.sh/keto/","ORY Keto")}. 
  The security is enforced at the gate, in the API micro-service that requires valid
  ${link("https://jwt.io","JSON Web Tokens")} for each RPC.
</p>

<h2>Monitoring</h2>
<p>
  The monitoring of the platform is achieved with
  ${link("https://prometheus.io","Prometheus")}. A set of exporters is in place for
  the gRPC services.
</p>
<p>
  The monitoring of the hosts is achieved via ${link("https://www.netdata.cloud","NetData")}.
  No data from Netdata is centralized in Prometheus.
</p>
